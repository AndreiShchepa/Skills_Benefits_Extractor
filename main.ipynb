{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentence-transformers thefuzz\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from abc import ABC, abstractmethod\n",
        "from nltk.corpus import stopwords\n",
        "from dataclasses import dataclass\n",
        "from transformers import logging\n",
        "import torch.nn.functional as F\n",
        "from bs4 import BeautifulSoup\n",
        "from thefuzz import fuzz\n",
        "from queue import Queue\n",
        "from nltk import ngrams\n",
        "from torch import nn\n",
        "import transformers\n",
        "import threading\n",
        "import torch\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "kaggle_dir = '/root/.kaggle'\n",
        "os.makedirs(kaggle_dir, exist_ok=True)\n",
        "\n",
        "! kaggle datasets download -d promptcloud/indeed-job-posting-dataset\n",
        "! kaggle datasets download -d thenoob69/esco-skills\n",
        "! kaggle datasets download -d ravindrasinghrana/job-description-dataset\n",
        "! mkdir datasets\n",
        "! unzip indeed-job-posting-dataset.zip -d datasets\n",
        "! unzip job-description-dataset.zip -d datasets\n",
        "! unzip esco-skills.zip -d datasets\n",
        "! rm indeed-job-posting-dataset.zip job-description-dataset.zip esco-skills.zip"
      ],
      "metadata": {
        "id": "eCRPQGjogkJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter One: Comparison of the Performance of Four Large Language Models on a Small Subset of Manually Labeled Data for the Task of Skills and Benefits Extraction"
      ],
      "metadata": {
        "id": "0zhs_UVrF2Nh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4_ZSM8vFPkd"
      },
      "outputs": [],
      "source": [
        "JOB_DESCRIPTIONS_FP = 'datasets/job_descriptions.csv'\n",
        "j_df = pd.read_csv(JOB_DESCRIPTIONS_FP)[['Benefits']].dropna().drop_duplicates()\n",
        "\n",
        "JOBS_FP = 'datasets/home/sdf/marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv'\n",
        "jd_df = pd.read_csv(JOBS_FP)[['Job Description']].dropna().drop_duplicates()\n",
        "\n",
        "ESCO_SKILLS_FP = 'datasets/skills.csv'\n",
        "es_df = pd.read_csv(ESCO_SKILLS_FP)[['label_cleaned']].dropna().drop_duplicates().rename(columns={'label_cleaned': 'Skills'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read labeled subset of data\n",
        "job_labels_file_path = 'job_labels.json'\n",
        "with open(job_labels_file_path, 'r') as file:\n",
        "    job_labels = json.load(file)"
      ],
      "metadata": {
        "id": "YyuQf1EMIvLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentence(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    words_no_stop = [word for word in words if word not in stop_words]\n",
        "    sentence_no_stop = \" \".join(words_no_stop)\n",
        "    return re.sub(r'[^\\w\\s]', '', sentence_no_stop)\n",
        "\n",
        "def get_sentences(jd):\n",
        "    soup = BeautifulSoup(jd, 'html.parser')\n",
        "    st = sent_tokenize(soup.get_text())\n",
        "    sentences = []\n",
        "\n",
        "    for sent in st:\n",
        "        sentences.extend([x for x in sent.split('\\n') if x != ''])\n",
        "\n",
        "    return [clean_sentence(s) for s in sentences]"
      ],
      "metadata": {
        "id": "4xAmFXktaSoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Items:\n",
        "    def __init__(self, benefits_df, skills_df, embedding_model):\n",
        "        self.benefits_df = benefits_df\n",
        "        self.skills_df = skills_df\n",
        "        self.embedding_model = embedding_model\n",
        "        self.emb_column_name = embedding_model.get_conf().emb_label\n",
        "        self.processed_skills_df = None\n",
        "        self.processed_benefits_df = None\n",
        "\n",
        "    def extract_items(self):\n",
        "        def extract_skills():\n",
        "            skills = self.skills_df['Skills'].apply(\n",
        "                lambda x: re.sub(r'\\([^)]*\\)', '', x)\n",
        "            ).str.strip()\n",
        "\n",
        "            self.processed_skills_df = pd.DataFrame({\n",
        "                'Original': skills,\n",
        "                'Skill': skills.apply(clean_sentence)\n",
        "            }).drop_duplicates('Skill').reset_index(drop=True)\n",
        "\n",
        "        def extract_benefits():\n",
        "            benefits_list = []\n",
        "            for row in self.benefits_df['Benefits']:\n",
        "                for benefit in re.sub(r'[\\{\\}\\']', '', row).split(','):\n",
        "                    original = ' '.join(benefit.split('(')[0].strip().lower().split())\n",
        "                    benefits_list.append((original, original))\n",
        "\n",
        "            # additional ChatGPT generated benefits\n",
        "            with open('benefits.txt', 'r') as file:\n",
        "                for line in file:\n",
        "                    benefit = line.strip()\n",
        "                    if benefit:\n",
        "                        standardized = ' '.join(benefit.lower().split())\n",
        "                        benefits_list.append((standardized, standardized))\n",
        "\n",
        "            self.processed_benefits_df = pd.DataFrame(\n",
        "                benefits_list,\n",
        "                columns=['Original', 'Benefit']\n",
        "            ).drop_duplicates('Benefit').reset_index(drop=True)\n",
        "\n",
        "        extract_skills()\n",
        "        extract_benefits()\n",
        "\n",
        "    def generate_embeddings(self, batch_size: int = 32):\n",
        "        def process_items_batch(df: pd.DataFrame, text_column: str):\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(df), batch_size):\n",
        "                batch = df[text_column].iloc[i:i + batch_size].tolist()\n",
        "                batch_embeddings = self.embedding_model.get_embeddings(batch)\n",
        "                embeddings.extend(batch_embeddings.numpy())\n",
        "\n",
        "            df[self.emb_column_name] = embeddings\n",
        "\n",
        "        process_items_batch(self.processed_skills_df, 'Skill')\n",
        "        process_items_batch(self.processed_benefits_df, 'Benefit')\n",
        "\n",
        "    def get_skills_df(self):\n",
        "        return self.processed_skills_df\n",
        "\n",
        "    def get_benefits_df(self):\n",
        "        return self.processed_benefits_df"
      ],
      "metadata": {
        "id": "BL-5znTaZWtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Configuration:\n",
        "    backbone_name: str\n",
        "    emb_label: str\n",
        "    tokenizer: AutoTokenizer\n",
        "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.backbone = AutoModel.from_pretrained(self.backbone_name).to(self.device)"
      ],
      "metadata": {
        "id": "rbtSjqfsZht9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel(ABC):\n",
        "    def __init__(self, backbone_name: str, emb_label: str):\n",
        "        self.conf = Configuration(\n",
        "            backbone_name=backbone_name,\n",
        "            emb_label=emb_label,\n",
        "            tokenizer=AutoTokenizer.from_pretrained(backbone_name)\n",
        "        )\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_embeddings(self, texts: list) -> torch.Tensor:\n",
        "        pass\n",
        "\n",
        "    def get_conf(self) -> Configuration:\n",
        "        return self.conf\n",
        "\n",
        "\n",
        "class JobBertModel(EmbeddingModel):\n",
        "    def __init__(self):\n",
        "        super().__init__(backbone_name='jjzha/jobbert-base-cased', emb_label='jobbert')\n",
        "\n",
        "    def get_embeddings(self, texts: list) -> torch.Tensor:\n",
        "        inputs = self.conf.tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(self.conf.device)\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.conf.backbone(**inputs).last_hidden_state[:, 0, :]\n",
        "        return embeddings.cpu()\n",
        "\n",
        "\n",
        "class MiniLMModel(EmbeddingModel):\n",
        "    def __init__(self):\n",
        "        self.conf = Configuration(\n",
        "            backbone_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "            emb_label='minilm',\n",
        "            tokenizer=None\n",
        "        )\n",
        "        self.model = SentenceTransformer(self.conf.backbone_name).to(self.conf.device)\n",
        "\n",
        "\n",
        "    def get_embeddings(self, texts: list) -> torch.Tensor:\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=False, convert_to_tensor=True, device=self.conf.device)\n",
        "        return embeddings.cpu()\n",
        "\n",
        "\n",
        "class DistilBertModel(EmbeddingModel):\n",
        "    def __init__(self):\n",
        "        super().__init__(backbone_name='distilbert-base-cased', emb_label='distilbert')\n",
        "\n",
        "    def get_embeddings(self, texts: list) -> torch.Tensor:\n",
        "        inputs = self.conf.tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(self.conf.device)\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.conf.backbone(**inputs).last_hidden_state[:, 0, :]\n",
        "        return embeddings.cpu()\n",
        "\n",
        "\n",
        "class RobertaModel(EmbeddingModel):\n",
        "    def __init__(self):\n",
        "        super().__init__(backbone_name='roberta-base', emb_label='roberta')\n",
        "\n",
        "    def get_embeddings(self, texts: list) -> torch.Tensor:\n",
        "        inputs = self.conf.tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(self.conf.device)\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.conf.backbone(**inputs).last_hidden_state[:, 0, :]\n",
        "        return embeddings.cpu()"
      ],
      "metadata": {
        "id": "wZ5-C5neZhpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(vecs, emb_type, df_items):\n",
        "    input_vectors = torch.stack(vecs).T\n",
        "    esco_vectors = torch.tensor(np.stack(df_items[emb_type].values), dtype=torch.float32)\n",
        "\n",
        "    norm_esco_vectors = torch.nn.functional.normalize(esco_vectors, p=2, dim=1)\n",
        "    norm_input_vectors = torch.nn.functional.normalize(input_vectors, p=2, dim=0)\n",
        "\n",
        "    cos_similarities = torch.matmul(norm_esco_vectors, norm_input_vectors)\n",
        "    sims, idxs = torch.max(cos_similarities, dim=0)\n",
        "\n",
        "    return idxs.numpy(), sims.numpy()"
      ],
      "metadata": {
        "id": "QpuHWOe3ZhiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_matches(matches):\n",
        "    unique_matches = {}\n",
        "\n",
        "    for ngram, skill_name, score in matches:\n",
        "        if (skill_name not in unique_matches or\n",
        "            score > unique_matches[skill_name][2]):\n",
        "            unique_matches[skill_name] = (ngram, skill_name, score)\n",
        "\n",
        "    return list(unique_matches.values())\n",
        "\n",
        "def is_match(el1, el2, threshold=0.8):\n",
        "    return fuzz.ratio(el1.lower(), el2.lower()) >= threshold * 100\n",
        "\n",
        "def calculate_metrics(matched, found, real):\n",
        "    tp = matched\n",
        "    fp = found - matched\n",
        "    fn = real - matched\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "def find_max_similarity(df_items, ngram_embs, emb_label, res, pred_labels, ngram_list, threshold=0.85):\n",
        "    idxs, sims = compute_similarity(ngram_embs, emb_label, df_items)\n",
        "    max_idx = np.argmax(sims)\n",
        "\n",
        "    if sims[max_idx] > threshold and idxs[max_idx] < len(df_items):\n",
        "        pred_labels.append(idxs[max_idx])\n",
        "        res.append((ngram_list[max_idx],\n",
        "                   df_items.iloc[idxs[max_idx]]['Original'],\n",
        "                   sims[max_idx]))\n",
        "\n",
        "def find_matched(unique_matches, expected):\n",
        "    matched = []\n",
        "    for _, name, score in unique_matches:\n",
        "        for expected_el in expected:\n",
        "            if is_match(name, expected_el):\n",
        "                matched.append((name, expected_el, score))\n",
        "\n",
        "    metrics = calculate_metrics(len(matched), len(unique_matches), len(expected))\n",
        "    return metrics['precision'], metrics['recall'], metrics['f1']"
      ],
      "metadata": {
        "id": "dJjmG0DHZroV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = .85\n",
        "\n",
        "for model in [MiniLMModel(), JobBertModel(), RobertaModel(), DistilBertModel()]:\n",
        "    processor = Items(\n",
        "        benefits_df=j_df,\n",
        "        skills_df=es_df,\n",
        "        embedding_model=model\n",
        "    )\n",
        "\n",
        "    processor.extract_items()\n",
        "    processor.generate_embeddings(batch_size=32)\n",
        "\n",
        "    skills_emb_df = processor.get_skills_df()\n",
        "    benefits_emb_df = processor.get_benefits_df()\n",
        "\n",
        "    metrics = {\n",
        "        'skills': {'acc': [], 'rec': [], 'f1': []},\n",
        "        'benefits': {'acc': [], 'rec': [], 'f1': []}\n",
        "    }\n",
        "\n",
        "    for job in job_labels:\n",
        "        idx, skills, benefits = job.values()\n",
        "        res_skills = []\n",
        "        pred_labels_skills = []\n",
        "        res_benefits = []\n",
        "        pred_labels_benefits = []\n",
        "        sentences = get_sentences(jd_df['Job Description'][int(idx)])\n",
        "        emb_label = model.get_conf().emb_label\n",
        "\n",
        "        for sent in sentences:\n",
        "            ngram_embs = []\n",
        "            sent = clean_sentence(sent)\n",
        "            sent_emb = model.get_embeddings(sent).squeeze()\n",
        "\n",
        "            words = sent.split()\n",
        "            ngram_max_len = 8 if len(words) > 8 else len(words)\n",
        "            ngram_list = [gram for i in range(1, ngram_max_len+1) for gram in list(ngrams(words, i))]\n",
        "\n",
        "            for ngram in ngram_list:\n",
        "                text = ' '.join(ngram)\n",
        "                ngram_embs.append((model.get_embeddings([text])[0])*0.8 + sent_emb*0.2)\n",
        "\n",
        "            if len(ngram_embs) == 0:\n",
        "                continue\n",
        "\n",
        "            find_max_similarity(benefits_emb_df, ngram_embs, emb_label,\n",
        "                                res_benefits, pred_labels_benefits, ngram_list)\n",
        "            find_max_similarity(skills_emb_df, ngram_embs, emb_label,\n",
        "                                res_skills, pred_labels_skills, ngram_list)\n",
        "\n",
        "        unique_matches_skills = get_unique_matches(res_skills)\n",
        "        unique_matches_benefits = get_unique_matches(res_benefits)\n",
        "\n",
        "        for type_, data in [('skills', (unique_matches_skills, skills)),\n",
        "                       ('benefits', (unique_matches_benefits, benefits))]:\n",
        "            acc, rec, f1 = find_matched(*data)\n",
        "            metrics[type_]['acc'].append(acc)\n",
        "            metrics[type_]['rec'].append(rec)\n",
        "            metrics[type_]['f1'].append(f1)\n",
        "\n",
        "        break\n",
        "\n",
        "    print(\"###############################################\")\n",
        "    print(f\"Model: {emb_label}\")\n",
        "    means = {k: {m: np.mean(v[m]) for m in v} for k, v in metrics.items()}\n",
        "    for category, values in means.items():\n",
        "        print(f\"\\n{category.capitalize()} means:\")\n",
        "        print('\\n'.join(f\"{k}: {v:.2f}\" for k, v in values.items()))"
      ],
      "metadata": {
        "id": "M50nv3coaP3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb88225-e00e-453f-896a-96f4974da99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###############################################\n",
            "Model: minilm\n",
            "\n",
            "Skills means:\n",
            "acc: 0.57\n",
            "rec: 0.40\n",
            "f1: 0.47\n",
            "\n",
            "Benefits means:\n",
            "acc: 1.00\n",
            "rec: 0.57\n",
            "f1: 0.73\n",
            "###############################################\n",
            "Model: jobbert\n",
            "\n",
            "Skills means:\n",
            "acc: 0.12\n",
            "rec: 0.30\n",
            "f1: 0.17\n",
            "\n",
            "Benefits means:\n",
            "acc: 0.67\n",
            "rec: 0.57\n",
            "f1: 0.62\n",
            "###############################################\n",
            "Model: roberta\n",
            "\n",
            "Skills means:\n",
            "acc: 0.08\n",
            "rec: 0.20\n",
            "f1: 0.11\n",
            "\n",
            "Benefits means:\n",
            "acc: 0.44\n",
            "rec: 0.57\n",
            "f1: 0.50\n",
            "###############################################\n",
            "Model: distilbert\n",
            "\n",
            "Skills means:\n",
            "acc: 0.11\n",
            "rec: 0.40\n",
            "f1: 0.17\n",
            "\n",
            "Benefits means:\n",
            "acc: 0.22\n",
            "rec: 0.57\n",
            "f1: 0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter Two: Optimization of Skills and Benefits Extraction for the Best-Performing Model"
      ],
      "metadata": {
        "id": "DQywXlXcGJQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Set\n",
        "import functools"
      ],
      "metadata": {
        "id": "TnRUSlqZNNC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ExtractionResult:\n",
        "    skills: Set[str]\n",
        "    benefits: Set[str]\n",
        "    confidence_scores: Dict[str, float]\n",
        "\n",
        "class PerfomanceMeasure(ABC):\n",
        "    def __init__(self, j_df, es_df, model):\n",
        "        self.model = model\n",
        "        self.prepare_embeddings(j_df, es_df)\n",
        "\n",
        "    def prepare_embeddings(self, j_df, es_df):\n",
        "        processor = Items(\n",
        "            benefits_df=j_df,\n",
        "            skills_df=es_df,\n",
        "            embedding_model=self.model\n",
        "        )\n",
        "\n",
        "        processor.extract_items()\n",
        "        processor.generate_embeddings(batch_size=32)\n",
        "\n",
        "        self.skills_emb_df = processor.get_skills_df()\n",
        "        self.benefits_emb_df = processor.get_benefits_df()\n",
        "        self.emb_label = self.model.get_conf().emb_label\n",
        "\n",
        "    def print_time(self, start, end, len_job_samples):\n",
        "        time_diff = end - start\n",
        "        if time_diff < 60:\n",
        "            print(f\"{len_job_samples} job descriptions were processed for {time_diff:.2f} seconds\")\n",
        "        else:\n",
        "            print(f\"{len_job_samples} job descriptions were processed for {time_diff/60:.2f} minutes\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def run(self, job_sample):\n",
        "        pass"
      ],
      "metadata": {
        "id": "tuzSlzPnaNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-optimized version from the first part"
      ],
      "metadata": {
        "id": "byqh2xWZYRQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequencePerfomance(PerfomanceMeasure):\n",
        "    def __init__(self, j_df, es_df, model):\n",
        "        super().__init__(j_df, es_df, model)\n",
        "\n",
        "    def run(self, job_samples):\n",
        "        start = time.time()\n",
        "        for job_sample in job_samples:\n",
        "            res_skills = []\n",
        "            res_benefits = []\n",
        "            pred_labels_skills = []\n",
        "            pred_labels_benefits = []\n",
        "            sentences = get_sentences(job_sample)\n",
        "\n",
        "            for sent in sentences:\n",
        "                ngram_embs = []\n",
        "                sent = clean_sentence(sent)\n",
        "                sent_emb = self.model.get_embeddings(sent).squeeze()\n",
        "\n",
        "                words = sent.split()\n",
        "                ngram_max_len = 8 if len(words) > 8 else len(words)\n",
        "                ngram_list = [gram for i in range(1, ngram_max_len+1) for gram in list(ngrams(words, i))]\n",
        "\n",
        "                for ngram in ngram_list:\n",
        "                    text = ' '.join(ngram)\n",
        "                    ngram_embs.append((self.model.get_embeddings([text])[0])*0.8 + sent_emb*0.2)\n",
        "\n",
        "                if len(ngram_embs) == 0:\n",
        "                    continue\n",
        "\n",
        "                find_max_similarity(self.benefits_emb_df, ngram_embs, self.emb_label,\n",
        "                                    res_benefits, pred_labels_benefits, ngram_list)\n",
        "                find_max_similarity(self.skills_emb_df, ngram_embs, self.emb_label,\n",
        "                                    res_skills, pred_labels_skills, ngram_list)\n",
        "\n",
        "            unique_matches_skills = get_unique_matches(res_skills)\n",
        "            unique_matches_benefits = get_unique_matches(res_benefits)\n",
        "\n",
        "        end = time.time()\n",
        "        self.print_time(start, end, len(job_samples))"
      ],
      "metadata": {
        "id": "V8W-e0g8af5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_samples = jd_df['Job Description'].sample(n=15, random_state=42)\n",
        "p = SequencePerfomance(j_df, es_df, MiniLMModel())\n",
        "p.run(job_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8PcnFRDLMT-",
        "outputId": "88854055-8324-44ab-8623-679f7aa9dd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 job descriptions were processed for 4.23 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized version with the following modifications:\n",
        "\n",
        "  * Batch Processing\n",
        "  * Adding LRU cache and pre-computed reference embeddings\n",
        "  * Parallel Processing\n",
        "  * Vectorized Similarity Computation\n",
        "\n"
      ],
      "metadata": {
        "id": "r8QxfIGpZLyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedSequencePerformance(PerfomanceMeasure):\n",
        "    def __init__(self, j_df, es_df, model, batch_size = 32):\n",
        "        super().__init__(j_df, es_df, model)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.skills_embeddings = self._prepare_reference_embeddings(self.skills_emb_df, self.emb_label)\n",
        "        self.benefits_embeddings = self._prepare_reference_embeddings(self.benefits_emb_df, self.emb_label)\n",
        "\n",
        "        self.sentence_cache = {}\n",
        "\n",
        "    def _prepare_reference_embeddings(self, df, emb_label):\n",
        "        \"\"\"Pre-compute and normalize reference embeddings.\"\"\"\n",
        "        embeddings = torch.stack([\n",
        "            torch.from_numpy(x) if isinstance(x, np.ndarray) else x\n",
        "            for x in df[emb_label]\n",
        "        ]).to(self.device)\n",
        "        return F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    @functools.lru_cache(maxsize=512)\n",
        "    def _get_cached_embedding(self, text):\n",
        "        \"\"\"Cache embeddings for frequently used text.\"\"\"\n",
        "        embedding = self.model.get_embeddings(text).squeeze()\n",
        "        return embedding.to(self.device)\n",
        "\n",
        "    def _process_batch(self, sentences):\n",
        "        return [self._get_cached_embedding(sent) for sent in sentences]\n",
        "\n",
        "    def _generate_ngrams_efficient(self, words, max_len = 8):\n",
        "        ngram_max_len = max_len if len(words) > max_len else len(words)\n",
        "        return list(ngrams(words, n) for n in range(1, ngram_max_len+1))\n",
        "\n",
        "    def _compute_similarities_batch(self, ngram_embeddings, reference_embeddings, threshold = 0.85):\n",
        "        \"\"\"Match sequence version normalization\"\"\"\n",
        "        with torch.no_grad():\n",
        "            input_vectors = ngram_embeddings.T\n",
        "            norm_input = F.normalize(input_vectors, p=2, dim=0)\n",
        "            norm_refs = F.normalize(reference_embeddings, p=2, dim=1)\n",
        "\n",
        "            similarities = torch.matmul(norm_refs, norm_input)\n",
        "            max_similarities, max_indices = torch.max(similarities, dim=0)\n",
        "\n",
        "            mask = max_similarities >= threshold\n",
        "            return max_similarities[mask], max_indices[mask]\n",
        "\n",
        "    def _generate_ngrams_efficient(self, words, max_len = 8):\n",
        "        \"\"\"Generate n-grams more efficiently using sliding window.\"\"\"\n",
        "        ngrams_dict = {}\n",
        "        words_len = len(words)\n",
        "        max_len = min(max_len, words_len)\n",
        "\n",
        "        for n in range(1, max_len + 1):\n",
        "            for i in range(words_len - n + 1):\n",
        "                ngram = tuple(words[i:i + n])\n",
        "                text = ' '.join(ngram)\n",
        "                if text not in ngrams_dict:\n",
        "                    ngrams_dict[text] = list(ngram)\n",
        "\n",
        "        return [(k, v) for k, v in ngrams_dict.items()]\n",
        "\n",
        "    def run(self, job_samples):\n",
        "        results = []\n",
        "        start = time.time()\n",
        "\n",
        "        for i in range(0, len(job_samples), self.batch_size):\n",
        "            batch = job_samples[i:i + self.batch_size]\n",
        "            batch_results = self._process_job_batch(batch)\n",
        "            # results.extend(batch_results)\n",
        "\n",
        "        end = time.time()\n",
        "        self.print_time(start, end, len(job_samples))\n",
        "        # return results\n",
        "\n",
        "    def _process_job_batch(self, job_batch):\n",
        "        batch_results = []\n",
        "\n",
        "        for job in job_batch:\n",
        "            sentences = get_sentences(job)\n",
        "            cleaned_sentences = [clean_sentence(sent) for sent in sentences]\n",
        "\n",
        "            # Process sentences in batches\n",
        "            sentence_embeddings = self._process_batch(cleaned_sentences)\n",
        "\n",
        "            job_skills = defaultdict(float)\n",
        "            job_benefits = defaultdict(float)\n",
        "\n",
        "            for sent_emb, sent in zip(sentence_embeddings, cleaned_sentences):\n",
        "                words = sent.split()\n",
        "                if not words:\n",
        "                    continue\n",
        "\n",
        "                ngrams = self._generate_ngrams_efficient(words)\n",
        "\n",
        "                if not ngrams:\n",
        "                    continue\n",
        "\n",
        "                ngram_texts = [text for text, _ in ngrams]\n",
        "                try:\n",
        "                    ngram_embeddings = torch.stack([\n",
        "                        self._get_cached_embedding(text) * 0.8 + sent_emb * 0.2\n",
        "                        for text in ngram_texts\n",
        "                    ]).to(self.device)\n",
        "                    ngram_embeddings = F.normalize(ngram_embeddings, p=2, dim=1)\n",
        "\n",
        "                    # Process skills and benefits\n",
        "                    skills_sims, skills_idx = self._compute_similarities_batch(\n",
        "                        ngram_embeddings, self.skills_embeddings)\n",
        "                    benefits_sims, benefits_idx = self._compute_similarities_batch(\n",
        "                        ngram_embeddings, self.benefits_embeddings)\n",
        "\n",
        "                    # Update results using correct column names\n",
        "                    for sim, idx in zip(skills_sims, skills_idx):\n",
        "                        skill = self.skills_emb_df.iloc[idx.item()]['Skill']\n",
        "                        job_skills[skill] = max(job_skills[skill], sim.item())\n",
        "\n",
        "                    for sim, idx in zip(benefits_sims, benefits_idx):\n",
        "                        benefit = self.benefits_emb_df.iloc[idx.item()]['Benefit']\n",
        "                        job_benefits[benefit] = max(job_benefits[benefit], sim.item())\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error processing ngrams for sentence: {sent[:100]}...\")\n",
        "                    continue\n",
        "\n",
        "            result = ExtractionResult(\n",
        "                skills=set(job_skills.keys()),\n",
        "                benefits=set(job_benefits.keys()),\n",
        "                confidence_scores={**job_skills, **job_benefits}\n",
        "            )\n",
        "            batch_results.append(result)\n",
        "\n",
        "        return batch_results"
      ],
      "metadata": {
        "id": "fh7fydxbZDpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_samples = jd_df['Job Description'].sample(n=15, random_state=42)\n",
        "p = OptimizedSequencePerformance(j_df, es_df, MiniLMModel())\n",
        "p.run(job_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW1DXNEJN9Z5",
        "outputId": "dad8c58a-b980-49b0-deaa-c43cb24ed15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 job descriptions were processed for 3.45 minutes\n"
          ]
        }
      ]
    }
  ]
}